{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccaa174",
   "metadata": {},
   "source": [
    "One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a465db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'picked': 0,\n",
       " 'seashore.': 1,\n",
       " 'of': 2,\n",
       " 'jumped': 3,\n",
       " 'she': 4,\n",
       " 'peck': 5,\n",
       " 'by': 6,\n",
       " 'seashells': 7,\n",
       " 'peppers.': 8,\n",
       " 'dog.': 9,\n",
       " 'lazy': 10,\n",
       " 'a': 11,\n",
       " 'piper': 12,\n",
       " 'over': 13,\n",
       " 'sells': 14,\n",
       " 'brown': 15,\n",
       " 'the': 16,\n",
       " 'peter': 17,\n",
       " 'fox': 18,\n",
       " 'pickled': 19,\n",
       " 'quick': 20}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"The quick brown fox jumped over the lazy dog.\",\n",
    "    \"She sells seashells by the seashore.\",\n",
    "    \"Peter Piper picked a peck of pickled peppers.\"\n",
    "]\n",
    "\n",
    "unique_words = set()\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        unique_words.add(word.lower())\n",
    "\n",
    "word_to_index = {}\n",
    "for i, word in enumerate(unique_words):\n",
    "    word_to_index[word] = i\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770b8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "one_hot_vectors = []\n",
    "for sentence in corpus:\n",
    "    sentence_vectors = []\n",
    "    for word in sentence.split():\n",
    "        vector = np.zeros(len(unique_words))\n",
    "        vector[word_to_index[word.lower()]] = 1\n",
    "        sentence_vectors.append(vector)\n",
    "    one_hot_vectors.append(sentence_vectors)\n",
    "\n",
    "for vector in one_hot_vectors[0]:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70326da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a4eb4d",
   "metadata": {},
   "source": [
    "One-Hot Encoding captures the presence or absence of words in a document but ignores the semantic relationship between words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c0cef",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112247e",
   "metadata": {},
   "source": [
    "BoW representation provides insights into the importance of different terms within the text. By counting the frequency of words, we can observe which words occur more frequently and, therefore, potentially carry more significance in the text. However, it still suffers from the sparsity problem and does not consider the semantic meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc65796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "data=pd.DataFrame({\"text\":[\"people watch ineuron\",\"ineuron watch ineuron\",\"people write comment\",\"ineuron write comment\"],\"output\":[1,1,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4c8adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'ineuron': 1, 'write': 4, 'comment': 0}\n",
      "['comment' 'ineuron' 'people' 'watch' 'write']\n",
      "[[0 1 1 1 0]\n",
      " [0 2 0 1 0]\n",
      " [1 0 1 0 1]\n",
      " [1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "BOW = CountVectorizer()\n",
    "document_matrix = BOW.fit_transform(data['text'])\n",
    "print(BOW.vocabulary_)\n",
    "print(BOW.get_feature_names_out())\n",
    "\n",
    "print(document_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa08a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram=CountVectorizer(ngram_range=(2,2))\n",
    "bigramvocab=bigram.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7728d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch': 2,\n",
       " 'watch ineuron': 4,\n",
       " 'ineuron watch': 0,\n",
       " 'people write': 3,\n",
       " 'write comment': 5,\n",
       " 'ineuron write': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee687b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people watch ineuron': 2,\n",
       " 'ineuron watch ineuron': 0,\n",
       " 'people write comment': 3,\n",
       " 'ineuron write comment': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram=CountVectorizer(ngram_range=(3,3))\n",
    "trigramvocab=trigram.fit_transform(data['text'])\n",
    "trigram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a05378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'people': 6,\n",
       " 'watch': 11,\n",
       " 'ineuron': 1,\n",
       " 'people watch': 7,\n",
       " 'watch ineuron': 12,\n",
       " 'people watch ineuron': 8,\n",
       " 'ineuron watch': 2,\n",
       " 'ineuron watch ineuron': 3,\n",
       " 'write': 13,\n",
       " 'comment': 0,\n",
       " 'people write': 9,\n",
       " 'write comment': 14,\n",
       " 'people write comment': 10,\n",
       " 'ineuron write': 4,\n",
       " 'ineuron write comment': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix=CountVectorizer(ngram_range=(1,3))\n",
    "mix_vocab=mix.fit_transform(data[\"text\"])\n",
    "mix.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a81d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75703634",
   "metadata": {},
   "source": [
    "Term Frequency (TF):\n",
    "The term frequency of a word within a document represents how frequently the word appears in that document.\n",
    "\n",
    "Inverse Document Frequency (IDF):\n",
    "The inverse document frequency measures the rarity or importance of a term in the entire collection of documents. \n",
    "\n",
    "TF-IDF still suffers from the lack of semantic meaning between words. It treats each word independently and does not consider the relationships or semantics between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bad3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3567743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.49681612, 0.61366674, 0.61366674, 0.        ],\n",
       "       [0.        , 0.8508161 , 0.        , 0.52546357, 0.        ],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027],\n",
       "       [0.61366674, 0.49681612, 0.        , 0.        , 0.61366674]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "tfidf.fit_transform(data[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c67225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['comment', 'ineuron', 'people', 'watch', 'write'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b2b0acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.51082562, 1.22314355, 1.51082562, 1.51082562, 1.51082562])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba33bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e301ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
